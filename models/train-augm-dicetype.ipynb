{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red_1d6', 'red_1d10', 'blue_1d20', 'blue_1d6']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('../own_dataset/diferent_dice/train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path= ('../own_dataset/diferent_dice/train')\n",
    "test_path= ('../own_dataset/diferent_dice/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10186 images belonging to 4 classes.\n",
      "Found 2002 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size_train=30  #,batch_size= batch_size_train\n",
    "batch_size_valid=30\n",
    "targetsize= 50\n",
    "datagen=ImageDataGenerator(rotation_range=180,height_shift_range=0.1,rescale=1.0/255.0\n",
    "                           ,zoom_range=[0.7,1.1],brightness_range=[0.4,1.2])\n",
    "train_batches= datagen.flow_from_directory(train_path, target_size=(targetsize,targetsize), \n",
    "                              classes=os.listdir('../own_dataset/diferent_dice/train'),\n",
    "                              batch_size= batch_size_train)\n",
    "test_batches= datagen.flow_from_directory(test_path,  target_size=(targetsize,targetsize), \n",
    "                              classes=os.listdir('../own_dataset/diferent_dice/train'),\n",
    "                              batch_size= batch_size_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots(ims, figsize=(20,10), rows=1, interp= False, titles= None):\n",
    "    if type(ims[0]) is np.ndarray:\n",
    "        ims = np.array(ims).astype(np.uint8)\n",
    "        if (ims.shape[-1] != 3):\n",
    "            ims= ims.transpose((0,1,2,3))\n",
    "    f= plt.figure(figsize=figsize)\n",
    "    cols= len(ims)//rows if len(ims) %2 == 0 else len(ims)//rows + 1\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, cols, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None:\n",
    "            sp.set_title(titles[i], fontsize=12)\n",
    "        plt.imshow(ims[i], interpolation=None if interp else 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 *0.95**x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nnfilters = [8, 16]\\nkernel_sizes = [(4,4), (2,2)]\\n\\n\\nmodel = Sequential()\\n# CONV1 (ReLU) > POOL1\\nmodel.add(Conv2D(nfilters[0], kernel_sizes[0], # 8 filters, 4x4\\n                 strides=(1,1),\\n                 padding='same', \\n                 input_shape=(targetsize,targetsize, 3),activation='relu'))\\nmodel.add(MaxPooling2D(pool_size=(8,8),\\n                       strides=(8,8),\\n                       padding='same'))\\n# CONV2 (ReLU) > POOL2\\nmodel.add(Conv2D(nfilters[1], kernel_sizes[1], # 16 filters, 2x2\\n                 strides=(1,1),\\n                 padding='same',activation='relu'))\\nmodel.add(MaxPooling2D(pool_size=(4,4),\\n                       strides=(4,4),\\n                       padding='same'))\\n\\n# Fully connected layer with softmax\\nmodel.add(Flatten())\\nmodel.add(Dense(6, activation='softmax'))\\nmodel.summary()\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "nfilters = [8, 16]\n",
    "kernel_sizes = [(4,4), (2,2)]\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# CONV1 (ReLU) > POOL1\n",
    "model.add(Conv2D(nfilters[0], kernel_sizes[0], # 8 filters, 4x4\n",
    "                 strides=(1,1),\n",
    "                 padding='same', \n",
    "                 input_shape=(targetsize,targetsize, 3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(8,8),\n",
    "                       strides=(8,8),\n",
    "                       padding='same'))\n",
    "# CONV2 (ReLU) > POOL2\n",
    "model.add(Conv2D(nfilters[1], kernel_sizes[1], # 16 filters, 2x2\n",
    "                 strides=(1,1),\n",
    "                 padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(4,4),\n",
    "                       strides=(4,4),\n",
    "                       padding='same'))\n",
    "\n",
    "# Fully connected layer with softmax\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.summary()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel.compile(Adam(lr=.0002), loss='categorical_crossentropy', metrics= ['accuracy'])\\nhistory= model.fit_generator(train_batches, steps_per_epoch= len(train_batches) , callbacks=[reduce_lr],\\n                             validation_data=train_batches, validation_steps= len(train_batches), \\n                             epochs=500)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "model.compile(Adam(lr=.0002), loss='categorical_crossentropy', metrics= ['accuracy'])\n",
    "history= model.fit_generator(train_batches, steps_per_epoch= len(train_batches) , callbacks=[reduce_lr],\n",
    "                             validation_data=train_batches, validation_steps= len(train_batches), \n",
    "                             epochs=500)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cp_callback=tf.keras.callbacks.ModelCheckpoint(filepath=os.getcwd(),save_weights_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('/home/ordovas/IRONHACK/dice-scores-recognition/model_d6_augmented_c1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 50, 50, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 25, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 25, 25, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 18436     \n",
      "=================================================================\n",
      "Total params: 111,684\n",
      "Trainable params: 111,684\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(targetsize,targetsize, 3)) )\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model2.add(Conv2D(filters=64, kernel_size=(3,3), padding='SAME', activation='relu'))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model2.add(Conv2D(filters=128, kernel_size=(3,3), padding='SAME', activation='relu'))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "#model2.add(Conv2D(filters=256, kernel_size=(3,3), padding='SAME', activation='relu'))\n",
    "#model2.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "#model2.add(Conv2D(filters=512, kernel_size=(3,3), padding='SAME', activation='relu'))\n",
    "#model2.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(len(os.listdir('../own_dataset/diferent_dice/train')), activation='softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-f6d32d39ba08>:2: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 601s 2s/step - loss: 0.2629 - accuracy: 0.9169 - val_loss: 0.0721 - val_accuracy: 0.9775\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 276s 813ms/step - loss: 0.0364 - accuracy: 0.9930 - val_loss: 0.0475 - val_accuracy: 0.9905\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 143s 422ms/step - loss: 0.0289 - accuracy: 0.9937 - val_loss: 0.0383 - val_accuracy: 0.9890\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 150s 442ms/step - loss: 0.0254 - accuracy: 0.9946 - val_loss: 0.0567 - val_accuracy: 0.9845\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 145s 427ms/step - loss: 0.0230 - accuracy: 0.9953 - val_loss: 0.0315 - val_accuracy: 0.9940\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 168s 495ms/step - loss: 0.0231 - accuracy: 0.9956 - val_loss: 0.0512 - val_accuracy: 0.9880\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 149s 438ms/step - loss: 0.0174 - accuracy: 0.9965 - val_loss: 0.0466 - val_accuracy: 0.9905\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 161s 473ms/step - loss: 0.0185 - accuracy: 0.9961 - val_loss: 0.0317 - val_accuracy: 0.9945\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 145s 425ms/step - loss: 0.0195 - accuracy: 0.9962 - val_loss: 0.0379 - val_accuracy: 0.9905\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 102s 300ms/step - loss: 0.0156 - accuracy: 0.9968 - val_loss: 0.0289 - val_accuracy: 0.9945\n"
     ]
    }
   ],
   "source": [
    "model2.compile(Adam(lr=.0002), loss='categorical_crossentropy', metrics= ['accuracy'])\n",
    "history2= model2.fit_generator(train_batches, steps_per_epoch= len(train_batches) , \n",
    "                             validation_data=test_batches, validation_steps= len(test_batches), \n",
    "                             epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "340/340 [==============================] - 105s 310ms/step - loss: 0.0127 - accuracy: 0.9973 - val_loss: 0.1315 - val_accuracy: 0.9630\n",
      "Epoch 2/3\n",
      "340/340 [==============================] - 106s 312ms/step - loss: 0.0129 - accuracy: 0.9964 - val_loss: 0.0406 - val_accuracy: 0.9920\n",
      "Epoch 3/3\n",
      "340/340 [==============================] - 109s 320ms/step - loss: 0.0147 - accuracy: 0.9969 - val_loss: 0.0205 - val_accuracy: 0.9960\n"
     ]
    }
   ],
   "source": [
    "history2= model2.fit_generator(train_batches, steps_per_epoch= len(train_batches) , \n",
    "                             validation_data=test_batches, validation_steps= len(test_batches), \n",
    "                             epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback=tf.keras.callbacks.ModelCheckpoint(filepath=os.getcwd(),save_weights_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('model_4difdices_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_json = model2.to_json()\n",
    "with open(\"model_4difdices_final.json\", \"w\") as json_file:\n",
    "    json_file.write(model2_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
